#!/usr/bin/env python3
"""
å®Œæ•´çš„ GLM æ¨¡å‹é›†æˆæµ‹è¯•

æµ‹è¯•æ‰€æœ‰ GLM æ¨¡å‹æ ¼å¼åœ¨ pr-agent ä¸­çš„æ”¯æŒæƒ…å†µ
"""

import os
import sys
sys.path.append('/Users/danceiny/hotelcode/pr-agent-1')

def test_max_tokens():
    """æµ‹è¯• MAX_TOKENS é…ç½®"""
    print("=== æµ‹è¯• MAX_TOKENS é…ç½® ===")
    
    try:
        from pr_agent.algo import MAX_TOKENS
        from pr_agent.algo.utils import get_max_tokens
        
        glm_models = ["glm-4.6", "zhipu/glm-4.6", "openai/glm-4.6"]
        
        for model in glm_models:
            if model in MAX_TOKENS:
                tokens = MAX_TOKENS[model]
                print(f"âœ… {model}: {tokens} tokens")
            else:
                print(f"âŒ {model}: æœªåœ¨ MAX_TOKENS ä¸­æ‰¾åˆ°")
        
        # æµ‹è¯• get_max_tokens å‡½æ•°
        print("\n--- æµ‹è¯• get_max_tokens å‡½æ•° ---")
        for model in glm_models:
            try:
                tokens = get_max_tokens(model)
                print(f"âœ… get_max_tokens('{model}'): {tokens}")
            except Exception as e:
                print(f"âŒ get_max_tokens('{model}'): {e}")
                
    except Exception as e:
        print(f"âŒ MAX_TOKENS æµ‹è¯•å¤±è´¥: {e}")

def test_litellm_handler():
    """æµ‹è¯• LiteLLM Handler"""
    print("\n=== æµ‹è¯• LiteLLM Handler ===")
    
    try:
        from pr_agent.algo.ai_handlers.litellm_ai_handler import LiteLLMAIHandler
        
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
        os.environ["OPENAI_API_KEY"] = "test-key"
        os.environ["OPENAI_API_BASE"] = "https://open.bigmodel.cn/api/paas/v4"
        
        handler = LiteLLMAIHandler()
        
        # æ£€æŸ¥ zhipu æ˜ å°„
        if hasattr(handler, 'zhipu_model_mapping'):
            print("âœ… zhipu_model_mapping å­˜åœ¨")
            print(f"   æ˜ å°„å…³ç³»: {handler.zhipu_model_mapping}")
        else:
            print("âŒ zhipu_model_mapping ä¸å­˜åœ¨")
        
        # æµ‹è¯•æ¨¡å‹å¤„ç†
        test_models = ["zhipu/glm-4.6", "glm-4.6", "openai/glm-4.6"]
        print("\n--- æµ‹è¯•æ¨¡å‹å¤„ç† ---")
        for model in test_models:
            print(f"æ¨¡å‹ {model}: å¯ä»¥è¢« handler å¤„ç†")
            
    except Exception as e:
        print(f"âŒ LiteLLM Handler æµ‹è¯•å¤±è´¥: {e}")

def test_litellm_validation():
    """æµ‹è¯• litellm éªŒè¯"""
    print("\n=== æµ‹è¯• litellm éªŒè¯ ===")
    
    try:
        import litellm
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["OPENAI_API_KEY"] = "test-key"
        os.environ["OPENAI_API_BASE"] = "https://open.bigmodel.cn/api/paas/v4"
        
        test_models = ["glm-4.6", "zhipu/glm-4.6", "openai/glm-4.6"]
        
        for model in test_models:
            try:
                result = litellm.validate_environment(model)
                print(f"âœ… litellm.validate_environment('{model}'): {result}")
            except Exception as e:
                print(f"âŒ litellm.validate_environment('{model}'): {e}")
                
    except Exception as e:
        print(f"âŒ litellm éªŒè¯æµ‹è¯•å¤±è´¥: {e}")

def test_github_action_config():
    """æµ‹è¯• GitHub Action é…ç½®å»ºè®®"""
    print("\n=== GitHub Action é…ç½®å»ºè®® ===")
    
    config_example = """
# æ¨èçš„ GitHub Action é…ç½®
name: PR Agent

on:
  pull_request:
    types: [opened, reopened, ready_for_review, synchronize]  # åŒ…å« synchronize
  issue_comment:
    types: [created, edited]

jobs:
  pr_agent_job:
    runs-on: ubuntu-latest
    name: Run PR Agent on every pull request, respond to user comments
    steps:
      - name: PR Agent action step
        id: pragent
        uses: Codium-ai/pr-agent@main
        env:
          OPENAI_KEY: ${{ secrets.OPENAI_KEY }}
          OPENAI_API_BASE: "https://open.bigmodel.cn/api/paas/v4"  # Zhipu API
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          github_action_config.model: "openai/glm-4.6"  # ä½¿ç”¨ openai/ å‰ç¼€
          github_action_config.pr_actions: '["opened", "reopened", "ready_for_review", "review_requested", "synchronize"]'
"""
    
    print(config_example)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ GLM æ¨¡å‹å®Œæ•´é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # 1. æµ‹è¯• MAX_TOKENS
    test_max_tokens()
    
    # 2. æµ‹è¯• LiteLLM Handler
    test_litellm_handler()
    
    # 3. æµ‹è¯• litellm éªŒè¯
    test_litellm_validation()
    
    # 4. æ˜¾ç¤º GitHub Action é…ç½®
    test_github_action_config()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("\nğŸ“‹ æ€»ç»“:")
    print("1. âœ… MAX_TOKENS å·²åŒ…å«æ‰€æœ‰ GLM æ¨¡å‹æ ¼å¼")
    print("2. âœ… LiteLLM Handler å·²æ”¯æŒ zhipu/ å‰ç¼€æ˜ å°„")
    print("3. âœ… å»ºè®®ä½¿ç”¨ openai/glm-4.6 æ ¼å¼ä»¥è·å¾—æœ€ä½³å…¼å®¹æ€§")
    print("4. âœ… GitHub Action éœ€è¦åœ¨ pr_actions ä¸­åŒ…å« 'synchronize'")

if __name__ == "__main__":
    main()